// Cargamos las variables de entorno de env
import * as dotenv from "dotenv";
dotenv.config();

// Importamos las librer√≠as necesarias
import { ChatGroq } from "@langchain/groq";
import { StateGraph, START, END } from "@langchain/langgraph";
import { z } from "zod";
import { TavilySearch } from "@langchain/tavily";
import readline from "readline";

// Flag de debug
const debug = true; // Con esto se controla si queremos mostrar los logs o solo el resultado final
// N√∫mero m√°ximo de mensajes de contexto
const MAX_HISTORY_MESSAGES = 10;

// Declaramos la memoria de la conversaci√≥n
let chatHistory = [];

// -----------------------------------------------
// Funci√≥n mejorada para extraer JSON
// Intenta parsear todo el string primero.
// Si falla, busca el primer bloque JSON v√°lido, incluso anidado.
// Lanza errores espec√≠ficos para mayor control.
// -----------------------------------------------
function extractJSON(str) {
  try {
    return JSON.parse(str);
  } catch (e) {
    // Busca el primer bloque JSON anidado
    const jsonRegex = /{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*}/s;
    const match = str.match(jsonRegex);
    if (match) {
      try {
        return JSON.parse(match[0]);
      } catch (e2) {
        throw new Error("JSON inv√°lido dentro del bloque");
      }
    }
    throw new Error("No se encontr√≥ JSON en la respuesta del LLM.");
  }
}

// Funci√≥n para leer input del usuario desde terminal
function promptUser(question) {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });
  return new Promise((resolve) => {
    rl.question(question, (answer) => {
      rl.close();
      resolve(answer);
    });
  });
}

async function main() {
  // Definimos el esquema de entrada y salida del grafo
  const schema = z.object({
    input: z.string(), // lo que le preguntas al bot
    intermediateSteps: z.array(z.any()).optional(),
    result: z.string().optional(), // respuesta final del bot
    // Si el LLM decide usar una herramienta, aqu√≠ se define la llamada
    tool_call: z.object({
      tool_name: z.string(),
      args: z.any(),
    }).optional(),
    next: z.string().optional(),
  });

  // Configuramos la herramienta Tavily para b√∫squeda web
  const tavilyTool = new TavilySearch({
    apiKey: process.env.TAVILY_API_KEY,
  });

  // Prompt que gu√≠a al LLM
  // Prompt reforzado para indicar al LLM que jam√°s escriba texto fuera del JSON.
  const systemMessage = `
Eres un asistente inteligente llamado JSBot y ordenado. Tu objetivo es ayudar al usuario de forma precisa y segura. Responde siempre en el idioma en que te preguntan.

**IMPORTANTE CR√çTICO**:
- ¬°NUNCA escribas texto FUERA del bloque JSON!
- Si incluyes texto adicional, el sistema fallar√°.
- Ejemplo INCORRECTO: "Pienso que... {\"action\": ...}"
- Ejemplo CORRECTO: {\"action\": ...}

- Siempre debes responder en formato JSON v√°lido.
- No escribas texto fuera del bloque JSON (ni explicaciones, ni comentarios, ni etiquetas como <think>).
- Si conoces la respuesta con seguridad, devu√©lvela directamente as√≠:

{"action":"finish","answer":"...respuesta..."}

- Si no est√°s completamente seguro de la respuesta o crees que necesitas informaci√≥n actualizada, debes usar la herramienta Tavily. Para ello, responde as√≠:

{
  "action":"use_tool",
  "tool":"tavily_search",
  "tool_input":{"query":"...texto a buscar..."}
}

- Si usas Tavily, incluye en tu respuesta final el enlace de la fuente principal que hayas usado (URL) para respaldar tu informaci√≥n.
`;

  // Definimos el nodo del LLM
  const chatbotNode = async (state) => {
    // Si estamos en modo debug, mostramos el estado actual
    if (debug) console.log("\nü§ñ Ejecutando LLM...");

    // Construimos el historial para mantener el contexto
    let historyText = "";
    if (chatHistory.length > 0) {
      // Nos quedamos con las √∫ltimas N entradas para no superar el l√≠mite de tokens
      const trimmedHistory = chatHistory.slice(-MAX_HISTORY_MESSAGES);
      historyText = "CONVERSACI√ìN ANTERIOR:\n";
      for (const message of trimmedHistory) {
        historyText += `- ${message.role === "user" ? "Usuario" : "Asistente"}: ${message.content}\n`;
      }
    }

    // Creamos el prompt incluyendo el historial y la pregunta actual
    let prompt = `${systemMessage}\n\n${historyText}\n\nPREGUNTA ACTUAL:\nUsuario: ${state.input}`;

    if (state.intermediateSteps?.length) {
      prompt += `\n\nInformaci√≥n obtenida de herramientas:\n${JSON.stringify(state.intermediateSteps, null, 2)}`;
    }

    // Instanciamos un modelo de Groq LLM
    const llm = new ChatGroq({
      apiKey: process.env.GROQ_API_KEY,
      model: "deepseek-r1-distill-llama-70b", // Modelo de Groq deepseek-r1-distill-llama-70b.
      temperature: 0.3, // Controla la aleatoriedad de las respuestas
      maxTokens: 1024, // M√°ximo de tokens en la respuesta del LLM
      maxRetries: 2, // Reintentos autom√°ticos en caso de errores transitorios
    });

    let response;
    try {
      // Invocamos el LLM con el prompt generado
      response = await llm.invoke(prompt);
    } catch (error) {
      console.error("‚ùå Error al invocar el LLM:", error);
      return {
        ...state,
        result: "Error: No se pudo invocar el LLM.",
        next: "END",
      };
    }

    // Si estamos en modo debug, mostramos la respuesta cruda del LLM
    if (debug) console.log("‚û°Ô∏è Respuesta RAW del LLM:\n", response.content);

    let parsed;
    try {
      // Usamos la funci√≥n extractJSON 
      parsed = extractJSON(response.content);

      // Validaci√≥n adicional de estructura
      // Solo se permiten las acciones "finish" o "use_tool"
      if (!parsed.action || !(parsed.action === "finish" || parsed.action === "use_tool")) {
        throw new Error("Estructura JSON inv√°lida o acci√≥n desconocida.");
      }
    } catch (e) {
      console.error("‚ùå Error parseando JSON:", e);

      //  En caso de error, limpiamos el historial para evitar loops de contexto corrupto
      chatHistory = [];

      return {
        ...state,
        result: "Disculpa, tuve un error interno. ¬øPodr√≠as reformular tu pregunta?",
        next: "END",
      };
    }

    // Mostramos si se ha decidido usar una herramienta o responder directamente
    if (parsed.action === "use_tool") {
      if (debug) console.log("üîß El LLM ha decidido usar la tool:", parsed.tool);
      return {
        //retornamos el estado actualizado con la llamada a la herramienta
        ...state,
        tool_call: {
          tool_name: parsed.tool,
          args: parsed.tool_input,
        },
        next: "tools",
      };
    } else if (parsed.action === "finish") {
      if (debug) console.log("‚úÖ El LLM ha decidido responder directamente.");

      // Guardamos en el historial
      chatHistory.push({ role: "user", content: state.input });
      chatHistory.push({ role: "assistant", content: parsed.answer });

      return {
        ...state,
        result: parsed.answer,
        next: "END",
      };
    } else {
      console.error("‚ö†Ô∏è Acci√≥n desconocida en la respuesta del LLM:", parsed);
      return {
        ...state,
        result: "Error: respuesta desconocida del LLM.",
        next: "END",
      };
    }
  };

  // Nodo de la herramienta
  const toolsNode = async (state) => {
    if (debug) console.log("\nüõ† Ejecutando tools...");
    // Si no hay tool_call, retornamos al chatbot
    let toolResult = null;

    if (state.tool_call?.tool_name === "tavily_search") {
      try {
        toolResult = await tavilyTool.invoke({
          query: state.tool_call.args.query,
        });
        if (debug) console.log("üîé Resultado Tavily:", toolResult);
      } catch (error) {
        console.error("‚ùå Error ejecutando Tavily:", error);
        toolResult = "Error: No se pudo ejecutar Tavily.";
      }
    } else {
      toolResult = "Error: Tool no reconocida.";
    }

    const steps = state.intermediateSteps || [];
    steps.push({
      tool_used: state.tool_call.tool_name,
      tool_output: toolResult,
    });

    return {
      ...state,
      intermediateSteps: steps,
      next: "chatbot",
    };
  };

  // Creamos el grafo pas√°ndole el schema.
  const graph = new StateGraph(schema);
  graph.addNode("chatbot", chatbotNode);
  graph.addNode("tools", toolsNode);
  graph.addEdge(START, "chatbot");
  graph.addConditionalEdges("chatbot", (state) => state.next);
  graph.addEdge("tools", "chatbot");
  graph.addEdge("chatbot", END);
  // Compilamos el grafo para que est√© listo para ejecutar.
  const executor = graph.compile();

  // Mensaje de bienvenida
  console.log(`
üëã ¬°Hola! Soy JSBot, tu asistente inteligente.

Puedo:
- responder preguntas generales
- buscar informaci√≥n en Internet si no s√© algo (uso Tavily Search)
- mantener el contexto de nuestra conversaci√≥n

Para finalizar la conversaci√≥n en cualquier momento, escribe: salir
`);
  // Entramos en bucle para seguir conversando
  while (true) {
    const userInput = await promptUser("\n> ");

    if (userInput.toLowerCase() === "salir") {
      console.log("¬°Hasta luego!");
      break;
    }

    const finalState = await executor.invoke({
      input: userInput,
    });

    if (debug) {
      console.log("\n Estado final:", finalState);
    }

    console.log("\n ü§ñ Respuesta :", finalState.result);
  }
}

// Llama a main y captura errores
main().catch(console.error);
